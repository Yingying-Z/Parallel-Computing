This document is intended to give some quick techniques for
implementing "embarrassingly parallel" computation on the Gizmo
cluster, maintained by Scientific Computing at FHCRC.
"Embarrassingly parallel(izable)" is actually a technical term.
(Look it up on Wikipedia.)  It means, roughly: A job that can
easily be broken down into independent parts which require little
or no interaction with each other.  Calculating null distributions
by permuting labels is embarrassingly parallel.  So is calculating
the value of function on different sections of its range.  In an
embarrassingly parallelizable situation, different computers could
all be used independently on different parts of the overall problem.

1) The Gizmo cluster is the set of computers we use for large scale
   parallel computing.  There are currently (Sept. 2014) about 1000
   compute cores available on Gizmo.  Peter Gilbert has about 200
   dedicated to our group.

   See: 

   https://teams.fhcrc.org/sites/citwiki/SciComp/Pages/Gizmo%20Cluster%20Quickstart.aspx

2) To use the Gizmo cluster, typically you log in to one of the Rhino 
   computers.  You don't typically log in to a gizmo computer directly.
   To log in to a Rhino computer, you can use a command like:

         ssh -X rhino.fhcrc.org

   or a terminal session with a program like putty. The Rhino computers
   take the same username and password as your email account.

   If you have trouble logging in to Rhino, send e-mail to the helpdesk
   at scientific computing at:

         scicomp@fhcrc.org 

3) The gizmo computers (and Rhino computers) have R (3.1.1) available.
   However, their standard libraries may not have all the packages you
   require for your work. You are not allowed to add or delete packages
   from the standard library.  However, you can add packages to your
   personal library.  After logging in to Rhino:

         rhino02> mkdir -p ~/R/library

   You only have to execute the mkdir command once in your entire life.
   It creates a place to put your personal copies of R packages.

         rhino02> R
         > install.packages("roxygen",lib="~/R/library")
         > 
         ...

   When you want to load the library:

         rhino02> R
         > require("roxygen",lib.loc="~/R/library")
         or
         > library("roxygen",lib.loc="~/R/library")

4) The package of programs used to access the Gizmo cluster from Rhino
   is broadly called "SLURM".  (see "man slurm").  There are about tweny
   programs in the package.  But you can get away with using only about
   three or four.

   sbatch  - can be used to submit an R job on the gizmo cluster.
   sacct   - can be used to observe it's state.
   scancel - can be used to kill a job or jobs that isn't doing what you want
   salloc  - can be used to communicate with a cluster job interactively.
   grabcpu - is a locally written script which will "log you in" to a
             gizmo computer.

   Use the "man" command to get help on any of these commands.  Examples
   will follow.

   Note that if you get a message like: "User's group not permitted to use
   this partition." -- contact the scicomp help desk and tell them.  Remind
   them that you belong to Peter Gilbert's group and should be able to use
   gilbert_p partition.

5) MPI (Message Passing Interface) is a simple protocol to do parallel
   computing.  The R package to use MPI is called Rmpi.  It is already 
   loaded in the standard library of the Rhino and Gizmo computers.  MPI
   and Rmpi is a very comprehensive package.  But you only need to deal
   with about five of the function calls.

   To load it, you use

   > require("Rmpi")

   MPI is a "master/slave" model.  A single "master" program controls 
   any number (hundreds!) of "slave" programs, and collates their work.

   It is convenient, but not strictly necessary, to have the master R 
   program and the slave R program in different files.

   In order for MPI to control slaves, the appropriate number of slave CPUs
   must already have been allocated by SLURM.  

   Here is a totally ridiculous way of generating 100 random normals:

   <sillytest.R>
   library("Rmpi")
   #Ask for 100 slaves
   mpi.spawn.Rslaves(nslaves=100)
   #Tell each slave to generate 1 random number and return the results
   #  into a list called "results"
   results <- mpi.remote.exec(rnorm(1),ret=TRUE)
   #shut down the slaves.  Delete the log file generated by each slave,
   #  otherwise you will end up with 100 log files.   
   mpi.close.Rslaves(dellog=TRUE)
   #save the results
   save(results,file="sillytest.RData")
   #quit, shutting down any pending slaves or requests
   mpi.quit(save="no")
    
   Here is a batch file to run sillytest.

   <sillytest.sh>
   #!/bin/sh
   R CMD BATCH sillytest.R

   Here is how to submit the batch job with 100 allocated CPUs:

   Rhino02> sbatch -N100 sillytest.sh

   Here is how to check the status of your batch job:

   Rhino02> sacct

   If all works as planned, you will have a file in your directory called
   "sillytest.RData".  It will contain one list (results), which contains 100 
   elements, each one a random normal.  You will also have a file called "
   sillytest.Rout" which will contain the output of your master R program.  
   And another file called "slurm<numbers>.log" which will probably be empty.  
   It would contain the output of your batch file, sillytest.sh, if any were 
   generated.  For example, if the commands it contained were incorrect.

6) Some random extras:

   * mpi.comm.rank() returns the sequence number of the slave. So, one way
     of generating a random seed per slave is:

     > seedThing <- as.numeric(Sys.time())+mpi.comm.rank()
     > mpi.spawn.Nl

   * sbatch can send you email when these events BEGIN, END, FAIL, REQUEUE,
     and ALL (any) happen. 
    
   * SLURM sets several environment variables which may be useful in your R 
     program.  For example, SLURM_NTASKS is the number of computers assigned
     to you. So

       > ncpus <- as.numeric(Sys.getenv("SLURM_NTASKS"))
       > mpi.spawn.Rslaves(nslaves=ncpus)
  
     will ascertain that you spawn as many slaves as you have computers.

   * mpi.bcast.cmd(<statement>) executes <statement> on every slave. 
     For example:

       > mpi.bcast.cmd(source("myFunctions.R")) 

     will load a copy of myFunctions.R onto every slave.

   * To run sillytest interactively on the cluster:

       Rhino02> grabcpu
       gizmo123> salloc -n 100 sh 
       gizmo123> R
       > source("sillytest.R")

       (control-C after R is finished to release gizmo nodes)

   * There are many options on the sbatch command.  You will possibly get faster
     access to resources if you specify ones that limit your CPU or clock time.
     Complementarily, there are default limits on cluster jobs.  And maximum 
     limits.  If you have a running job that approaches the time limits, you will
     get e-mail from scientific computing which will warn you that your job is 
     about to be canceled. You can request an extension.  But it is a good idea
     to pre-specify the amount of time your job will take, if you know it, or can
     make a good guess.   
